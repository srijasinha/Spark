spark is designed to support wide range of task over the same computing engine. It works on master slave architecture.
Master - It divides the task to diffrent computers(slave)
For ex - Data scientist, Data Enginner, Data Anylyst all can use the same platform for there modeling and treansformations.
a)Computing engine - spark is limited to a  computing engine. It doesn't store the data. spark can connect to diffrent data sources HDFS,JDBC,Azure storage etc.
b)Pallel processing - Parallel computing is a computing architecture that divides a problem into smaller tasks and runs them concurrently.
c)Apache spark - Apache spark is an unified computing engine and set of libraries for pallel data processing on computer cluster.

Q)Why we need Apache Spark?
Ans- Earlier we have Database - Oracle,TeraData,MySQL. we can store only Structured data.
But Now a days - a)data is generated in csv files,test file,json ,image,video etc .To handle these files we don't have any system. 
b)Volume of data is increased 
Big Data - 3'V of Big Data - 1)Velocity - 1/hr,1/sec 
                             2)Variety - structured,semi structured ,un-structured 
                             3)Volume- 10TB
10 Tb of data is introduced in 1hr then its a Big data
ETL happens before Data Ware house concept , but now ELT happens(Data Lake)
 Note -***Issue - 1)Storage issue of data
                  2)Processing - A)RAM B)CPU
To solve this issue we have two options
a)Monolithhic appoch - To make a computer system big (To scale up hardware, but it has maximum capacity) - vertical scaling,Expensive, Low  avaibility
b)Disributed approch -Horizonal Scaling, High availbilty,Economical

To solve distributed system approch Hadoop came abfter that spark came
Hadoop->Spark

Q)Hadoop and Spark diffrence?
Ans - Hadoop - a) Hadoop is a framework. It is slower than spark because it writes the data back to disk again and read again from from disk to in-memory.
b)It work on mapper and reducer
c)Build for Batch Data Processing
d)Difficult to write code in Hadoop.Hive was built to make it easier.
e)Have solid security feature
f)Fault torerance - It is having block of data and replication factor to handle the faliure.


Spark -a)Spark is faster than Hadoop because spark do all the computation in memory.(RAM)
Ex - Data -- a)Executer - Executer(RAM)
             b)Executer - Executer(RAM)
b)Build for Batch as well as Streaming data Processing
c)Easy to write and debug code.We have interactive shell to develop and test high level and low level API.
d)Don't have solid security feature.
e)Uses DAG (Directed Acyclic graph) [its one after another not in cycle] to provide fault tolerance. Ex- Process1 -Process2-Process3 
If process 2 fails it will create a process 2 again and give, spark knows how data is built for Process2

***Spark Eco-system***
Dataframe spark sql - streaming -Ml lib - GraphX - (Libraies High -Level API) - Dtaframe is generated
Spark core -Python,java,R,Scala (Low lvel API) - RDD is generated
Spark engine 
clusture Manager
no of computers(3)(FIFO)

***Spark clusture **
Cluster - No of computers connected through Network is called cluster
Node/computer [Driver program)(spark context)->cluster manager->worker Node(2)
Driver program (Master) will break the code into smalller tasks.It will send the information to our cluster manager.Cluster Manager will create the worker Nodes(Slaves) bases on information 
and that worker nodes will actually process our data.This is called master slave architecture.
Developer needs to run spark application , a developer need - 20 GGB Driver,Executer- 25 GB, No of executer - 5 GB,CPU Core - 5
Master will say to slave - please create a container(w1) of 5GM RAM 
container (w1)- 5 GB RAM its called Application Master(pyhton, Pyspark)(main method)(Pyspark driver)--->Internally called jvm method(java)(called application Driver)
Afer Applicatin master - Executer1(python),E2(JVM)

Q)What is transformation and actions?
Ans- When we make any process on data then its called transformation.EX- join,Filter,Group By these are transformations.
There are two types of transformation - 
1)Wide transformations - Data movement is not done to find any value.(like filter,select,union)
2)Narrow transformations - Data movement is done to find any value.(to perform group by,join,distincit)
Actions are like count,show,collect

***DAG -Directed Acyclic Graph 
DAG is created for each JOB
Job is created when action is hit
NOTE - For each action 1 job is created, and for each JOB has its own DAG.

Lazy evaluation ---
It refers to the strategy where transformations on distributed datasets are not executed immediately
Instead, Spark builds up a sequence of transformations that are only executed when an action is called. Here’s how it works:

1. Transformation Phase: When you apply transformations (e.g., map, filter) to an RDD/DataFrame/Dataset, 
Spark doesn’t process the data immediately. Instead, it records these transformations and builds a logical execution plan,
known as a DAG (Directed Acyclic Graph).

2. Action Phase: The actual computation only occurs when an action (e.g., collect, count,show) is called.
At this point, Spark takes the DAG, applies various optimizations, and executes the transformations in an efficient manner.

---Spark SQL Engine- 
SQL,DataFrame,Dataset-->Catylat Optimizer,SQL Spark Engine ---Rdd(Physical Plan)
4 Phases of Spark sql engine
1)Anylysis phase
2)Logical Planning
3)Physical Planning
4)Code Generation

--RDD-Resilent Distributed DataSet -Its a Data structure in spark.It is immutable (cant get changed).It is lazy optimization
Data is distributed on cluster and give data is RDD.List got converted in RDD and dristributed on multiple cluster.
Resilent - In case of falure ,it recovers the data
Distributed- Data is distributed on cluster and give data.
Dataset-Actual data
We need RDD - 1)Full contron on our data
              2)Unstructured

Q)In Apache Spark both SparkContext and SparkSession serve as entry points to the Spark cluster, but they differ in their purpose and use cases. 
SparkContext is the foundational entry point for all Spark functionality, particularly for low-level operations with RDDs (Resilient Distributed Datasets). 
SparkSession, introduced in Spark 2.0, provides a higher-level, unified interface for working with DataFrames and Datasets, and is recommended for most data
processing tasks.

----Below steps indicates the workflow of Spark SQL execution

1)JDBC/ODBC and console clients like spark shell execute a Spark SQL query using DataFrame/Dataset API
2)Data in DataFrame/Dataset gets processed and optimized using catalyst optimizer
3)Catalyst optimizer executes the query to a physical plan, optimizes and compiles parts of queries to Java byte code
4)The optimized byte code constructs RDDs as per the query plan which eventually gets executed within Spark engine as a DAG
5)The output will be persisted as a file format supported by data source API

---Below steps explains the workflow of catalyst optimizer-

1)Analyzing a logical plan with the metadata
2)Optimizing the logical plan
3)Creating multiple physical plans
4)Analyzing the plans and find the most optimal physical plan
5)Converting the physical plan to RDDs

----Parquet file--
1)Columnar file format supported across many data processing systems. Provides better performance while storing and processing data
2)Stores schema details along with the data in the file
3)Default file format supported by Spark Framework
4)Spark SQL offers API functions to read/write parquet files

----Avro file format--
1)Avro file format provides high performance in read, write operations and optimized storage in compressed format.
2)Uses JSON to define datatypes and protocols. JSON is the standard serialization component, across businesses for representing data.
3)Avro serialize data in a binary format with the help of JSON. 
4)Spark SQL supports reading and writing Avro files using the spark-avro library






                                         


       
